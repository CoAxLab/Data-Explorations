
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Feature hacking &#8212; Data explorations</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/brain.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data explorations</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Data explorations
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Phase 1 - Assessing the scene
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/art-of-data-investigations.html">
   Art of data investigations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../art-of-data-investigations.html">
     Art of data investigations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/the-value-of-openness.html">
   The value of openness
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../the-value-of-openness.html">
     The value of openness
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/the-value-of-openness.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/what-is-a-theory.html">
   What is a theory?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/what-is-a-theory.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/constructing-a-testable-hypothesis.html">
   Constructing a testable hypothesis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/constructing-a-testable-hypothesis.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Phase 2 - Evaluating the evidence
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/data-as-objects-and-architectures.html">
   Data as objects and architectures
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-as-objects-and-architectures.html">
     Data as objects and architectures
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/data-as-objects-and-architectures.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/techniques-for-data-cleansing.html">
   Techniques for data cleansing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../techniques-for-data-cleansing.html">
     Techniques for data cleansing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/techniques-for-data-cleansing.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/visualization-as-analysis.html">
   Visualization as analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../visualization-as-analysis.html">
     Visualization as analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/visualization-as-analysis.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/a-visualization-vocabulary.html">
   A visualization vocabulary
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/a-visualization-vocabulary.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/what-is-learnable.html">
   What is learnable?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="simple">
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/the-bias-variance-tradeoff.html">
   The bias-variance tradeoff
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/the-bias-variance-tradeoff.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/linear-models.html">
   Linear models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../linear-models.html">
     Linear models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/linear-models.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/the-ordinary-least-squares-solution.html">
   The ordinary least squares solution
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../the-ordinary-least-squares-solution.html">
     The ordinary least squares solution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/the-ordinary-least-squares-solution.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/limits-of-linear-regression.html">
   Limits of linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../limits-of-linear-regression.html">
     Limits of linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/limits-of-linear-regression.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/mixed-effects-models.html">
   Mixed effects models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../mixed-effects-models.html">
     Mixed-effects models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/mixed-effects-models.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/classifiers.html">
   Classifiers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../classifiers.html">
     Classifiers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/classifiers.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/the-beauty-of-knn.html">
   The beauty of kNN
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../the-beauty-of-knn.html">
     The beauty of kNN
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/cross-validation.html">
   Cross validation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../cross-validation.html">
     Cross-validation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/cross-validation.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/resampling-methods.html">
   Resampling methods
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../resampling-methods.html">
     Resampling methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/resampling-methods.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/power-analysis-via-simulations.html">
   Power analysis via simulations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../power-analysis-via-simulations.html">
     Power analysis via simulations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/power-analysis-via-simulations.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/mediation-and-moderation.html">
   Mediation and moderation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
  <label for="toctree-checkbox-20">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../mediation-and-moderation.html">
     Mediation and moderation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/mediation-and-moderation.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/selecting-the-best-model.html">
   Selecting the best model
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
  <label for="toctree-checkbox-21">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../selecting-the-best-model.html">
     Selecting the best model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/selecting-the-best-model.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/regularized-regression.html">
   Regularized regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
  <label for="toctree-checkbox-22">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../regularized-regression.html">
     Regularized regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/regularized-regression.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/principal-component-methods.html">
   The value of openness
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
  <label for="toctree-checkbox-23">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../principal-component-methods.html">
     Principal component methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/principal-component-methods.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Phase 3 - Solving the mystery
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/reconsidering-the-p-value.html">
   Reconsidering the p-value
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/>
  <label for="toctree-checkbox-24">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/reconsidering-the-p-value.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/bayes-factor-accepting-the-null.html">
   Bayes factor: accepting the null
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/>
  <label for="toctree-checkbox-25">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../bayes-factor-accepting-the-null.html">
     Bayes factor: accepting the null
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/bayes-factor-accepting-the-null.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../lectures/telling-your-data-story.html">
   Telling your data story
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/>
  <label for="toctree-checkbox-26">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../discussions/telling-your-data-story.html">
     Discussion questions:
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/notebooks/old/Bonus-Feature_Hacking.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/CoAxLab/Data-Explorations/main?urlpath=tree/book/notebooks/old/Bonus-Feature_Hacking.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/CoAxLab/Data-Explorations/blob/main/book/notebooks/old/Bonus-Feature_Hacking.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Feature hacking
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#goals-of-the-lab">
     Goals of the lab:
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-selection-with-cross-validation">
   Feature selection with cross validation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Feature hacking
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example">
     Example
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#detecting-hacked-fits">
     Detecting hacked fits
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#take-home-message">
     Take home message
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Feature hacking</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Feature hacking
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#goals-of-the-lab">
     Goals of the lab:
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-selection-with-cross-validation">
   Feature selection with cross validation
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Feature hacking
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example">
     Example
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#detecting-hacked-fits">
     Detecting hacked fits
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#take-home-message">
     Take home message
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="feature-hacking">
<h1>Feature hacking<a class="headerlink" href="#feature-hacking" title="Permalink to this headline">¶</a></h1>
<div class="section" id="goals-of-the-lab">
<h2>Goals of the lab:<a class="headerlink" href="#goals-of-the-lab" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Learn about feature selection with cross-validation.</p></li>
<li><p>Learn about feature hacking.</p></li>
</ul>
<p>The lecture draws from Chapters 5 of James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). “An introduction to statistical learning: with applications in r.”</p>
</div>
</div>
<hr class="docutils" />
<div class="tex2jax_ignore mathjax_ignore section" id="feature-selection-with-cross-validation">
<h1>Feature selection with cross validation<a class="headerlink" href="#feature-selection-with-cross-validation" title="Permalink to this headline">¶</a></h1>
<p>In our previous lectures, we mentioned mulitple ways to select the appropriate features (i.e., predictor variables) in your model that balances the bias-variance tradeoff. We focused on two general approaches:</p>
<ol class="simple">
<li><p>Subset/stepwise selection</p></li>
<li><p>Shrinkage models</p></li>
</ol>
<p>When discussing feature selection with subset/stepwise methods, we showed how you can evaluate models using bias-adjusted terms (e.g., AIC). When discussing shrinkage models, the regularization term, <span class="math notranslate nohighlight">\(\lambda\)</span>, which does the feature selection, was shown to be selected using cross-valiation. However, cross-validation can also be used with subset/stepwise selection methods as well. It just increases the computational time, but allows for a direct test of the bias-variance tradeoff.</p>
<p>Thus, cross validation can be seen as a widely useful tool for both feature selection and fitting models.</p>
<p>If you remember from our previous lecture on cross-validation, we stated that there are two goals of using validation sets:</p>
<ol class="simple">
<li><p>Evaluate not just whether <span class="math notranslate nohighlight">\(X\)</span> is meaningful with relationship to <span class="math notranslate nohighlight">\(Y\)</span> but to what degree it explains variance in <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
<li><p>Evaluate the <strong>external validity</strong> of your model. External validity is the extent to which the results of a study can be genralized to other situations.</p></li>
</ol>
<br>
<p>By having independent training and test sets, we can evaluate the various forms of <span class="math notranslate nohighlight">\(X\)</span> (i.e., the different combinations of variables in <span class="math notranslate nohighlight">\(X\)</span>), to identify the model that most effectively manages the bias-variance tradeoff.</p>
<p><img alt="Variance-bias tradeoff" src="notebooks/old/imgs/L6TrainVsTest.png" /></p>
<p>Put another way, we can use validation sets to select the appropriate features (variables in <span class="math notranslate nohighlight">\(X\)</span>) that explain enough variance in <span class="math notranslate nohighlight">\(Y\)</span>, while still retaining the ability to predict future observations.</p>
<p>For this to work properly, the data used to train the model (<span class="math notranslate nohighlight">\(X\)</span> &amp; <span class="math notranslate nohighlight">\(Y\)</span>) must be <em>fully independent</em> from the data used to evalute test error (<span class="math notranslate nohighlight">\(X'\)</span> &amp; <span class="math notranslate nohighlight">\(Y'\)</span>). Usually <span class="math notranslate nohighlight">\(Y'\)</span> is considered sacrosanct and left completely untouched (though for unsupervised models <span class="math notranslate nohighlight">\(X'\)</span> is just as sacred). In many areas of statistics and machine learning, researchers are not even given access to <span class="math notranslate nohighlight">\(Y'\)</span> at all and must submit their predictions to an independent arbitrator (e.g., the course instructor, organizers of hackathon).</p>
<p>Thus the general form for k-fold cross-validation is:</p>
<br>
<ul class="simple">
<li><p>Step 1: Take a set of <span class="math notranslate nohighlight">\(m\)</span> observations (<span class="math notranslate nohighlight">\(m=\frac{n}{k}\)</span>). Make a training data set, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> from these <span class="math notranslate nohighlight">\(m\)</span> observations.</p></li>
<li><p>Step 2: Make a new data set <span class="math notranslate nohighlight">\(X'\)</span> and <span class="math notranslate nohighlight">\(Y'\)</span> from the remaining data outside the selected <span class="math notranslate nohighlight">\(m\)</span>.</p></li>
<li><p>Step 3: Fit <span class="math notranslate nohighlight">\(Y = \hat{\beta}_0 + \hat{\beta}_1 X^*\)</span>. Call this <span class="math notranslate nohighlight">\(\hat{f}()\)</span>.</p></li>
<li><p>Step 4: Calculate the accuracy (e.g., classification accuracy) or goodness-of-fit (e.g., RSS) of <span class="math notranslate nohighlight">\(\hat{f}()\)</span> against <span class="math notranslate nohighlight">\(Y'\)</span> by applying <span class="math notranslate nohighlight">\(X'\)</span> (i.e., <span class="math notranslate nohighlight">\(\hat{Y'} = \hat{f}(X')\)</span>).</p></li>
<li><p>Step 5: Repeat Steps 1-4 for all k folds.</p></li>
<li><p>Step 6: Calculate the average performance metric across all k-folds.</p></li>
</ul>
 <br>
<p>When setup properly, cross-validation gives you an approximation of how well your model generalizes to future data. It gives a sense of what would happen if you tried running a replication experiment. It is worth noting that this is only an approximation. A real replication experiment might have different sources of noise (e.g., different backgrounds of the participants, different lighting conditions, different social contexts). Whereas, in cross-validation, the noise is generally the same across folds because you are sampling from the same set of observations.</p>
<p>Beyond this limitation, cross-validation is emerging as a gold standard for statistical models in the social and neural sciences. If an effect can survive cross-validation, then it is likely robust enough to survive replication tests and follow up studies. It is seen as being much more robust that p-values and less sensitive to “hacking”.</p>
<p>But in reality, there is a weak point in cross-validation that makes it sensitive to hacking. A violation of a single principle that can inflate your test set evaluation performance.</p>
<p>That weakness is the purity of independence between the portion of your data set that you use to select your features and the portion of your data set you use to learn the model.</p>
</div>
<hr class="docutils" />
<div class="tex2jax_ignore mathjax_ignore section" id="id1">
<h1>Feature hacking<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>Any violation of this independence between <span class="math notranslate nohighlight">\(X,Y\)</span> and <span class="math notranslate nohighlight">\(X',Y'\)</span> leads to contamination in your test evaluation (Step 4 above). Which means that variance in your training set influences test set accuracy.</p>
<p>One way that this violation of independence arises is in the feature selection process itself. For example, including <span class="math notranslate nohighlight">\(X',Y'\)</span> in the tuning of <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span> for sparse regression methods or running subset selection on your entire data set and then breaking the data up into test and training sets. Even if the error, after feature selection, is evaluted using cross-validation, the feature selection was done on both the test and training set.</p>
<p>This reflects what we are calling <strong>feature hacking</strong> and is a classic overfitting problem in machine learning. Feature hacking is the non-principled process of selecting the best subset of the data that maximizes performance of the hold out test set used to determine prediction accuracy. This inflates model performance by finding the unique features that explain the most variance in the hold out test set. This process produces a model with the highest variance (i.e., flexibility), and lowest bias, which also means it produces a model with the least generalizability to future data.</p>
<p>Remember, that feature selection is supposed to be a conservative process. You might discard out terms that are meaningful in explaining variance in the test set because you are trying to constrain model variability. Feature hacking, does the opposite. It chooses the best combination of features that can explain the unique variance in your test set that would otherwise be excluded.</p>
<p>In other words, feature hacking over-fits on the test set.</p>
<p>As was mentioned in the section above. The goal of cross-validation is to approximate doing a validation <em>experiment</em>. Feature hacking is the equivalent of giving you clairvoyance to see what form your model should take in the future validation experiment you haven’t yet run.</p>
<div class="section" id="example">
<h2>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h2>
<p>To see this more clearly, let’s consider an example. We will simulate a data set of 100 observations (<span class="math notranslate nohighlight">\(n\)</span>) and 40 predictor variables (<span class="math notranslate nohighlight">\(p\)</span>). We will use uniform sampling to randomly determine the true regression weights (<span class="math notranslate nohighlight">\(\beta 's\)</span>) that link <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>In this example we will take 30 observations as our tuning set to estimate <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Setup data set</span>
<span class="n">n</span> <span class="o">=</span> <span class="m">100</span>
<span class="n">p</span> <span class="o">=</span> <span class="m">40</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="m">7</span> <span class="c1"># Standard deviation of the noise term</span>

<span class="c1"># Generate X and Y</span>
<span class="n">X</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span><span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">p</span><span class="p">),</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">betas</span> <span class="o">=</span> <span class="nf">runif</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">min</span> <span class="o">=</span> <span class="m">-1</span><span class="p">,</span> <span class="n">max</span> <span class="o">=</span> <span class="m">1</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">%*%</span> <span class="n">betas</span> <span class="o">+</span> <span class="nf">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">sd</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">)</span>

<span class="c1"># Generate up a tuning and training set</span>
<span class="n">tune_id</span> <span class="o">&lt;-</span> <span class="nf">sample</span><span class="p">(</span><span class="nf">nrow</span><span class="p">(</span><span class="n">Y</span><span class="p">),</span> <span class="m">30</span><span class="p">)</span>
<span class="n">tune_X</span> <span class="o">&lt;-</span> <span class="n">X[tune_id</span><span class="p">,</span><span class="n">]</span>
<span class="n">tune_Y</span> <span class="o">&lt;-</span> <span class="n">Y[tune_id]</span>

<span class="n">train_X</span> <span class="o">&lt;-</span> <span class="n">X[</span><span class="o">-</span><span class="n">tune_id</span><span class="p">,</span><span class="n">]</span>
<span class="n">train_Y</span> <span class="o">&lt;-</span> <span class="n">Y[</span><span class="o">-</span><span class="n">tune_id]</span>
</pre></div>
</div>
</div>
</div>
<p>Next let’s use LASSO to do feature selection on our model. Since <span class="math notranslate nohighlight">\(\hat{\lambda}\)</span> is the term that does the feature selection, we will select it in two separate ways. First we will determine <span class="math notranslate nohighlight">\(\lambda\)</span> using the full data set. This is our “hacked” example, so we will call this value <span class="math notranslate nohighlight">\(\lambda_{hacked}\)</span>. Then we will estimate <span class="math notranslate nohighlight">\(\lambda\)</span> the appropriate way, by only using the tuning set.</p>
<p>Performance in the training set will be estimated using cross-validation with each <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># libraries</span>
<span class="nf">library</span><span class="p">(</span><span class="n">glmnet</span><span class="p">)</span>

<span class="c1"># First use all the data to find lambda</span>
<span class="n">model.hacked</span> <span class="o">=</span> <span class="nf">cv.glmnet</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s">&quot;gaussian&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="m">1</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">model.hacked</span><span class="p">)</span>
<span class="n">lambda_hacked</span> <span class="o">=</span> <span class="n">model.hacked</span><span class="o">$</span><span class="n">lambda.min</span>

<span class="c1"># Now use only the tuning set</span>
<span class="n">model.tune</span> <span class="o">=</span> <span class="nf">cv.glmnet</span><span class="p">(</span><span class="n">tune_X</span><span class="p">,</span> <span class="n">tune_Y</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s">&quot;gaussian&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="m">1</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">model.tune</span><span class="p">)</span>
<span class="n">lambda_tune</span> <span class="o">=</span> <span class="n">model.tune</span><span class="o">$</span><span class="n">lambda.min</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Bonus-Feature_Hacking_6_0.png" src="../../_images/Bonus-Feature_Hacking_6_0.png" />
<img alt="../../_images/Bonus-Feature_Hacking_6_1.png" src="../../_images/Bonus-Feature_Hacking_6_1.png" />
</div>
</div>
<p>Notice the difference in values we get.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print</span><span class="p">(</span><span class="n">lambda_hacked</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">lambda_tune</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] 0.5748486
[1] 1.324828
</pre></div>
</div>
</div>
</div>
<p>Now let’s see how the LASSO fits on the training set look using the two different <span class="math notranslate nohighlight">\(\lambda's\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">hacked.fit</span> <span class="o">=</span> <span class="nf">glmnet</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s">&quot;gaussian&quot;</span><span class="p">,</span> <span class="n">lambda</span><span class="o">=</span><span class="n">lambda_hacked</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="m">1</span><span class="p">)</span>
<span class="n">tune.fit</span> <span class="o">=</span> <span class="nf">glmnet</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s">&quot;gaussian&quot;</span><span class="p">,</span> <span class="n">lambda</span><span class="o">=</span><span class="n">lambda_tune</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="m">1</span><span class="p">)</span>

<span class="c1"># The number of non-zero coefficients</span>
<span class="n">n_hacked_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="nf">coef</span><span class="p">(</span><span class="n">hacked.fit</span><span class="p">)</span><span class="o">!=</span><span class="m">0</span><span class="p">)</span>
<span class="n">n_tune_params</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="nf">coef</span><span class="p">(</span><span class="n">tune.fit</span><span class="p">)</span><span class="o">!=</span><span class="m">0</span><span class="p">)</span>

<span class="c1"># Model fits</span>
<span class="n">hacked.rss</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">((</span><span class="n">train_Y</span> <span class="o">-</span> <span class="nf">predict</span><span class="p">(</span><span class="n">hacked.fit</span><span class="p">,</span><span class="n">newx</span><span class="o">=</span><span class="n">train_X</span><span class="p">))</span><span class="n">^2</span><span class="p">)</span>
<span class="n">tune.rss</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">((</span><span class="n">train_Y</span> <span class="o">-</span> <span class="nf">predict</span><span class="p">(</span><span class="n">tune.fit</span><span class="p">,</span><span class="n">newx</span><span class="o">=</span><span class="n">train_X</span><span class="p">))</span><span class="n">^2</span><span class="p">)</span>

<span class="n">hacked.pred</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="n">hacked.fit</span><span class="p">,</span> <span class="n">newx</span><span class="o">=</span><span class="n">train_X</span><span class="p">)</span>
<span class="n">tune.pred</span> <span class="o">=</span> <span class="nf">predict</span><span class="p">(</span><span class="n">tune.fit</span><span class="p">,</span> <span class="n">newx</span><span class="o">=</span><span class="n">train_X</span><span class="p">)</span>

<span class="nf">plot</span><span class="p">(</span><span class="n">train_Y</span><span class="p">,</span> <span class="n">hacked.pred</span><span class="p">)</span>
<span class="n">hacked.cor</span> <span class="o">=</span> <span class="nf">cor</span><span class="p">(</span><span class="n">train_Y</span><span class="p">,</span> <span class="n">hacked.pred</span><span class="p">)</span>
<span class="n">hacked.rss</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">((</span><span class="n">train_Y</span><span class="o">-</span><span class="n">hacked.pred</span><span class="p">)</span><span class="n">^2</span><span class="p">)</span>


<span class="nf">plot</span><span class="p">(</span><span class="n">train_Y</span><span class="p">,</span> <span class="n">tune.pred</span><span class="p">)</span>
<span class="n">tune.cor</span> <span class="o">=</span> <span class="nf">cor</span><span class="p">(</span><span class="n">train_Y</span><span class="p">,</span> <span class="n">tune.pred</span><span class="p">)</span>
<span class="n">tune.rss</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">((</span><span class="n">train_Y</span><span class="o">-</span><span class="n">tune.pred</span><span class="p">)</span><span class="n">^2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Bonus-Feature_Hacking_10_0.png" src="../../_images/Bonus-Feature_Hacking_10_0.png" />
<img alt="../../_images/Bonus-Feature_Hacking_10_1.png" src="../../_images/Bonus-Feature_Hacking_10_1.png" />
</div>
</div>
<p>Notice that our hacked model is doing better than our properly tuned model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">cat</span><span class="p">(</span><span class="s">&quot;hacked&quot;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">hacked.cor</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">hacked.rss</span><span class="p">)</span>

<span class="nf">cat</span><span class="p">(</span><span class="s">&quot;\ntuned&quot;</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">tune.cor</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">tune.rss</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>hacked            s0
[1,] 0.7811447
[1] 2062.844

tuned           s0
[1,] 0.582562
[1] 3446.85
</pre></div>
</div>
</div>
</div>
<p>The reason our hacked model is doing so well is that, by using the entire data set together to estimate out the sparsity constraint, we are including variance from the test data, <span class="math notranslate nohighlight">\(X'\)</span> and <span class="math notranslate nohighlight">\(Y'\)</span> in the feature selection process. In other words we are selecting features based on how well they will perform in <strong>both</strong> the test and tuning sets.</p>
</div>
<hr class="docutils" />
<div class="section" id="detecting-hacked-fits">
<h2>Detecting hacked fits<a class="headerlink" href="#detecting-hacked-fits" title="Permalink to this headline">¶</a></h2>
<p>One way to detect the presence of feature hacking (or over-fitting in general) is to evaluate the likelihood of finding a better model on the training set.</p>
<p>Remember, proper tuning and cross-validation is conservative. Which means that it is actually missing a lot of unique variance in the training set. Thus there are a lot of other configurations of the model (i.e., variables included in <span class="math notranslate nohighlight">\(X\)</span>) that would do <strong>better</strong> in terms of explaining variance in <span class="math notranslate nohighlight">\(Y'\)</span>.</p>
<p>However, if you hack the feature selection process, then the probability of finding a better combination of predictor variables (for the same model complexity), is very low.</p>
<p>We can look at this in the simulations we ran before. The hacked model return the following number of features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print</span><span class="p">(</span><span class="n">n_hacked_params</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] 17
</pre></div>
</div>
</div>
</div>
<p>This determines our model complexity. We can now randomly select among the 40 original variables in <span class="math notranslate nohighlight">\(X\)</span>, choosing this many at random, and see how well we do compared to both the hacked and the properly tuned models we fit above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Permutation test</span>
<span class="n">n_iterations</span> <span class="o">=</span> <span class="m">1000</span>
<span class="n">rss</span> <span class="o">=</span> <span class="nf">matrix</span><span class="p">(</span><span class="kc">NaN</span><span class="p">,</span><span class="n">nrow</span><span class="o">=</span><span class="n">n_iterations</span><span class="p">,</span><span class="n">ncol</span><span class="o">=</span><span class="m">1</span><span class="p">)</span>

<span class="nf">for </span><span class="p">(</span><span class="n">i</span> <span class="n">in</span> <span class="m">1</span><span class="o">:</span><span class="n">n_iterations</span><span class="p">){</span>
    <span class="n">p2use</span> <span class="o">=</span> <span class="nf">sample</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">p</span><span class="p">)</span><span class="n">[1</span><span class="o">:</span><span class="n">n_hacked_params]</span>
    <span class="n">new_X</span> <span class="o">=</span> <span class="n">train_X[</span><span class="p">,</span><span class="n">p2use]</span>
    <span class="n">glm.fit</span> <span class="o">=</span> <span class="nf">glm</span><span class="p">(</span><span class="n">train_Y</span><span class="o">~</span><span class="n">new_X</span><span class="p">,</span> <span class="n">family</span><span class="o">=</span><span class="s">&quot;gaussian&quot;</span><span class="p">)</span>
    <span class="n">rss[i]</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">glm.fit</span><span class="o">$</span><span class="n">residuals^2</span><span class="p">)</span>
 <span class="p">}</span>

<span class="nf">hist</span><span class="p">(</span><span class="n">rss</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">hacked.rss</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="o">=</span><span class="n">tune.rss</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Bonus-Feature_Hacking_17_0.png" src="../../_images/Bonus-Feature_Hacking_17_0.png" />
</div>
</div>
<p>In the graph above, the red line represents the RSS we got from our hacked model and the blue line shows the RSS for the properly tuned model. Notice how conservative proper tuning is at explaining variance in our model.</p>
<p>The probabilities of finding a better model configuration for either the hacked or tuned models (respectively) are:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">print</span><span class="p">(</span><span class="nf">mean</span><span class="p">(</span><span class="n">rss</span><span class="o">&lt;</span><span class="n">hacked.rss</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">mean</span><span class="p">(</span><span class="n">rss</span><span class="o">&lt;</span><span class="n">tune.rss</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1] 0
[1] 0.904
</pre></div>
</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="take-home-message">
<h2>Take home message<a class="headerlink" href="#take-home-message" title="Permalink to this headline">¶</a></h2>
<p>The feature selection process can contribute to over-fitting as well. When applied in a cross-validation process, you may think that you are measuring the unbiased generalization performance of your model. However, you would be wrong. If you include the observations from your test set in the feature selection process, then you are automatically biasing your model to the unique variance in that data set.</p>
<p>In otherwords, cross validation does not (itself) guarantee against overfitting.</p>
<p><em>Notebook authored by Tim Verstynen and edited by Amy Sentis.</em></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            kernelName: "ir",
            path: "./notebooks/old"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Timothy Verstynen, Venn Popov, Krista Bond, Charles Wu, Patience Stevens, Amy Sentis<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>